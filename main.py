import streamlit as st
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import networkx as nx
import plotly.graph_objects as go

st.set_page_config(page_title="ë‚˜ë…¸ìœµí•©ê¸°ìˆ  ìœ ì‚¬ë„ ë„¤íŠ¸ì›Œí¬", layout="wide")
st.title("ğŸ”¬ ë‚˜ë…¸ìœµí•©ê¸°ìˆ  100ì„  â€“ ìœ ì‚¬ë„ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™”")

# âœ… GitHub CSV URL
csv_url = "https://raw.githubusercontent.com/gpig0702/20025.06.02/main/kimm_nano_100.csv"

try:
    df = pd.read_csv(csv_url, encoding="utf-8")
    st.success("âœ… CSV íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
except Exception as e:
    st.error(f"âŒ CSV ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨: {e}")
    st.stop()

# âœ… ê¸°ìˆ  ì„¤ëª… ì»¬ëŸ¼ ì„ íƒ
text_col = st.selectbox("ê¸°ìˆ  ì„¤ëª…ì´ í¬í•¨ëœ ì—´ì„ ì„ íƒí•˜ì„¸ìš”", df.columns)

# âœ… ìœ ì‚¬ë„ ì„ê³„ê°’ ìŠ¬ë¼ì´ë”
threshold = st.slider("ìœ ì‚¬ë„ ì„ê³„ê°’ (0.0 ~ 1.0)", 0.0, 1.0, 0.3, 0.05)

# âœ… ê²€ìƒ‰ì–´ ì…ë ¥
search_query = st.text_input("ğŸ” ê¸°ìˆ  ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì„¼ì„œ)", "").strip()

# âœ… TF-IDF ë²¡í„°í™”
texts = df[text_col].fillna("").astype(str).tolist()
vectorizer = TfidfVectorizer()
try:
    tfidf_matrix = vectorizer.fit_transform(texts)
except Exception as e:
    st.error(f"TF-IDF ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
    st.stop()

similarity_matrix = cosine_similarity(tfidf_matrix)

# âœ… ë„¤íŠ¸ì›Œí¬ ìƒì„±
G = nx.Graph()
for i, txt in enumerate(texts):
    G.add_node(i, label=txt[:6] + "â€¦", full_text=txt)  # ë¼ë²¨ ë” ì§§ê²Œ

for i in range(len(texts)):
    for j in range(i + 1, len(texts)):
        if similarity_matrix[i][j] >= threshold:
            G.add_edge(i, j, weight=float(similarity_matrix[i][j]))

if G.number_of_nodes() == 0:
    st.warning("ë°ì´í„°ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
    st.stop()
if G.number_of_edges() == 0:
    st.warning("ê°„ì„ ì´ ì—†ìŠµë‹ˆë‹¤. ì„ê³„ê°’ì„ ë‚®ì¶°ë³´ì„¸ìš”.")
    st.stop()

# âœ… ì¢Œí‘œ ë° ì‹œê°í™” ë°ì´í„° ìƒì„±
pos = nx.spring_layout(G, seed=42)
node_x, node_y, hover_texts, short_labels, node_sizes, node_colors = [], [], [], [], [], []

highlighted_nodes = []

for n in G.nodes():
    x, y = pos[n]
    full_text = G.nodes[n]["full_text"]
    label = G.nodes[n]["label"]

    node_x.append(x)
    node_y.append(y)
    hover_texts.append(full_text)
    short_labels.append(label)

    if search_query and search_query.lower() in full_text.lower():
        node_colors.append("red")
        node_sizes.append(20)
        highlighted_nodes.append((n, full_text))
    else:
        node_colors.append("#8dbbf2")
        node_sizes.append(8)

edge_x, edge_y = [], []
for e in G.edges():
    x0, y0 = pos[e[0]]
    x1, y1 = pos[e[1]]
    edge_x += [x0, x1, None]
    edge_y += [y0, y1, None]

# âœ… ì‹œê°í™” (ê¸€ì í¬ê¸° ì¡°ì ˆ í¬í•¨)
edge_trace = go.Scatter(
    x=edge_x, y=edge_y, mode="lines",
    line=dict(width=0.5, color="#ccc"), hoverinfo="none"
)

node_trace = go.Scatter(
    x=node_x, y=node_y, mode="markers+text",
    text=short_labels, textposition="top center",
    hovertext=hover_texts, hoverinfo="text",
    textfont=dict(size=9),  # âœ… ê¸€ì í¬ê¸° ì¤„ì„
    marker=dict(
        color=node_colors,
        size=node_sizes,
        line=dict(width=1, color="black")
    )
)

fig = go.Figure(data=[edge_trace, node_trace],
                layout=go.Layout(
                    title=dict(text="ê¸°ìˆ  ìœ ì‚¬ë„ ê¸°ë°˜ ë„¤íŠ¸ì›Œí¬", font=dict(size=20)),
                    showlegend=False,
                    hovermode="closest",
                    margin=dict(b=20, l=5, r=5, t=40),
                    xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),
                    yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)
                ))

st.plotly_chart(fig, use_container_width=True)

# âœ… ë³´ì¡° ë¶„ì„ - ê²€ìƒ‰ ê²°ê³¼
if search_query:
    st.subheader("ğŸ” ê²€ìƒ‰ ê²°ê³¼ ëª©ë¡")
    if highlighted_nodes:
        matched_df = pd.DataFrame(highlighted_nodes, columns=["Index", "ê¸°ìˆ  ì„¤ëª…"])
        st.dataframe(matched_df.set_index("Index"))
    else:
        st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

# âœ… [ì‹ ê·œ ê¸°ëŠ¥] ìœ ì‚¬ë„ ìƒìœ„ ê¸°ìˆ ìŒ + ê³µí†µ í‚¤ì›Œë“œ
st.subheader("ğŸ“Š ìœ ì‚¬ë„ ìƒìœ„ ê¸°ìˆ  ìŒ + ìœ ì‚¬ í‚¤ì›Œë“œ")

similarities = []
for i in range(len(texts)):
    for j in range(i + 1, len(texts)):
        similarities.append((i, j, similarity_matrix[i, j]))

top_similar_pairs = sorted(similarities, key=lambda x: x[2], reverse=True)[:10]

feature_names = vectorizer.get_feature_names_out()

def get_top_shared_keywords(i, j, top_n=5):
    vec_i = tfidf_matrix[i].toarray().flatten()
    vec_j = tfidf_matrix[j].toarray().flatten()
    avg_scores = (vec_i + vec_j) / 2
    shared_keywords_idx = np.argsort(avg_scores)[::-1][:top_n]
    return [feature_names[k] for k in shared_keywords_idx]

for idx1, idx2, sim in top_similar_pairs:
    keywords = get_top_shared_keywords(idx1, idx2)
    st.markdown(f"""
    âœ… **ê¸°ìˆ  A:** {df[text_col][idx1]}  
    âœ… **ê¸°ìˆ  B:** {df[text_col][idx2]}  
    ğŸ“Œ **ìœ ì‚¬ë„:** {sim:.3f}  
    ğŸ”‘ **ê³µí†µ í‚¤ì›Œë“œ:** _{', '.join(keywords)}_  
    ---
    """)
